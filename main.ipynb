{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d92d9d",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906cad9f",
   "metadata": {},
   "source": [
    "#### ¿Qué pasa si algunas acciones tienen probabilidades de cero?\n",
    "##### R. Que algunas acciones tengan probabilidad de cero significa que durante la etapa de exploración nunca se llegó a explorar la posibilidad de tomar dicha acción en el estado en cuestión. El agente nunca la seleccionará pero no significa que sea una mala acción, simplemente no tiene registro del outcome o reward de tomarla.\n",
    "\n",
    "#### ¿Qué pasa si la póliza es determinística?\n",
    "####    a. π1(a) = 1 para algún a\n",
    "##### R. Significa que el agente siempre tomará la misma acción dado un estado, causando que el resto de acciones sean ignoradas a menos que la política sea actualizada. \n",
    "\n",
    "#### Investigue y defina a qué se le conoce como cada uno de los siguientes términos, asegúrese de definir en qué consiste cada una de estas variaciones y cómo difieren de los k-armed bandits:\n",
    "- Contextual bandits\n",
    "    ##### R. Como se puede inferir del nombre, Contextual bandits es una modificación al algoritmo de k-armed bandits donde se implementa un contexto adicional para convertirse en un algoritmo más personalizado. Esto se obtiene a través de utilizar una matriz mxn en vez de un array n de 1d. Dado el contexto se tomaría la acción con mejor reward en la matriz. (Gupta, 2023)\n",
    "- Dueling bandits\n",
    "    ##### R. Al igual que el anterior, del nombre puede inferirse la logica de este nuevo problema de bandits. En vez de solo tomar un arm o brazo, se toman dos y tienen un 'duelo' para ver cuál es mejor en vez de estimar las recompensas de cada brazo. Para este problema se asume, comunmente, un ordenamiento de los brazos tal que si $i \\succ j$, entonces $P_{ij} > \\frac{1}{2}$. Este proceso daría dos nuevas nociones de pérdida:\n",
    "    ![Pérdidas](images/duelling_regrets.PNG)\n",
    "\n",
    "- Combination bandits\n",
    "    ##### R. En este problema de bandits, en vez de escoger solo un 'arm', se escoge un subset o combinación de arms del set. El objetivo es maximizar la recompensa de la combinación. La noción de pérdida es la siguiente:\n",
    "    ![Pérdida](images/combinatorial_regret.PNG)\n",
    "\n",
    "Lattimore, T., & Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. https://banditalgs.com/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
